{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Applied Deep Learning Tutorial \n# Segmentation with U-Net\n\n## Introduction\nThis tutorial presents a segmentation use-case based on U-Net network (see paper for reference [paper](https://arxiv.org/pdf/1505.04597.pdf)).\nFirst, the theoretical background behind the architecture is discussed. Subsequently you will be guided through the implementation process of the neural network within this Jupyter Notebook. The use-case is based on the initial paper and focuses on cell boundary segmentation, see Fig.1.\n<img src=\"graphics/unet_fig_1.png\" width=\"700\"><br>\n<center> Figure 1: Right: input image | Left: Prediction of the cell boundaries </center>","metadata":{}},{"cell_type":"markdown","source":"## Motivation\n\nWhile segmentation networks and convolutional neural networks often need a very large amount of training data in order to converge, it is necessary to deploy the training data more efficiently. In segmentation, objects need to be localized and classified. This requires improved semantic understanding. \nThe U-Net achieves sample efficiency while capturing both class affiliations and localization performance.  ","metadata":{}},{"cell_type":"markdown","source":"## What is U-Net\n\n<img src=\"graphics/unet_fig_2.png\" width=\"700\"><br>\n<center> Figure 2: U-Net architecture </center>\n\nIn Fig.2, the architecture of the U-Net is shown, and it is seen by the shape of the architecture why it is called U-Net.\nTo understand the U-Net we will walk through the layers step by step, starting with the input.\nThe input consist out of image's tile. Because only a tile is processed by the network the image can be of an arbitrarily large size and for example it size is not linked to the memory of your GPU. An example of such a tile can be seen in Fig.3 below.\n\n<img src=\"graphics/unet_fig_3.png\" width=\"300\">\n\n<center> Figure 3: Tile of an image </center>\n\nFor the yellow part, the center of the tile, the network predict a labeled output whilst the blue region is the region which is fed in to the neural network. Due to this difference there is a overlap-tile strategry to obtaine predicted labels for the whole image and avoid redundant informations.<br> Because a nonpadded image is used with the convolutions, it leads to a 1-pixel loss around the image. \nThe convolutions marked with the blue arrows within Fig.2 process the input by building feature maps of the shown size. These convolution a ReLu function is followed. After the first 3 convolutions a feature map of the size 568x568x64 is received. Thereafter max-pooling operation (2x2 with a stride of 2), marked with red arrows in the image, downsample the image size but simultaneously doubles the size of the feature map.<br>\nThis is repeated until we get a single vector with a high resolution feature map. Thus this contracting path reduces the information of the location by simultaneously increase the context information (features). From this high resolution feature vector, now a high resolution segmentation-map with a high accuracy of the location is built. This is done by expand the vector with up-sampling, due up-convolution and with, and this is the novelty, concatenate the high resolution feature map from the contracting path. This is shown with the grey arrows seen in the Fig.2. Furthermore after every upconcolution a ReLU operation is appended. \nWith this combination of knotting the upconcolution of the high resolution feature map with the localization information, it is not getting loss as it is the case with just do an \"autoencoding\". <br>\nDue to the aforementioned loss of information with the nonpadded convolution, the feature map needs to be cropped.\nThe final layer consists out of a 1x1 convolution which maps each 64 component feature vector to the desired number of classes. And thus devide the image into the requested segments. ","metadata":{}},{"cell_type":"markdown","source":"## Implementing the code \n\nWe will orientate with this tutorial on the implemtation of the code from [this](https://github.com/zhixuhao/unet) github repository.\nDue to the implementation in Jupyter Notebook we will write the whole code in one template. Instead of different python documents we will use classes.\nThe original repository consists out of three python documents. To work only with this notebooky every function is integrated here, thus you don't have to switch documents or import other python-files.\n","metadata":{}},{"cell_type":"code","source":"# If you do not have the data yet, you can download it here:\n# https://github.com/zhixuhao/unet/tree/master/data/membrane\n# Now go to the folder in which this jupyter Notebook ist located and running\n# and copy the <<data>> folder there and renaming either the folders according\n# to the Path written below or change the path written below according to\n# your folder-names\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! git clone https://github.com/zhixuhao/unet.git\n! cp -r unet/data data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When you run the Code below and you set the Path accordingly, an image and its label should be shown.","metadata":{}},{"cell_type":"code","source":"##########################################################\n#           In this box some code must be added          #\n##########################################################\nfrom IPython.display import Image\n\n# We want to see one of the training data with its label\n# Change the folders or the code that the path points to your data\ntraining_images = \"data/membrane/train/image/\"\ntraining_labels = \"data/membrane/train/label/\"\n\n# We use the function Image(file_name) to print the image '0.png' and the label 'o.png'\n### Insert Code ###\ndisplay(Image(training_images + \"0.png\"))\ndisplay(Image(training_labels + \"0.png\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Buidling the Unet Architecture\n\nIn the next box all libraries we will need for the Unet and its dataprepration will be imported. Because Keras is a part of tensorflow and is installed within, we do not have to install in seperately and though can also start it with <br>\n\"tensorflow.keras\"\n\nIf something went wrong with importing the packages, you may try to restart your kernel.","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function\n\nimport glob\nimport os\n\nimport numpy as np\nimport skimage.io as io  # pip install scikit-image pip install scipy\n\n# if This Error occurs:\n# <<ImportError: cannot import name 'rgb2gray'>>\n# restart your kernel\nimport skimage.transform as trans\nimport tensorflow as tf\nfrom keras import Sequential\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.utils import image_dataset_from_directory","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To build the archtecture which is shown in fig.1 keras is a very intuitive and simply way to do so. We will slithly adapt the archtecture from the original one like seen in fig.3. To build this neural net We do need the following functions:\n- 2D Convolution\n- MaxPooling \n- Concatenation\n\nWe will add an Dropout after convolution 4 and 5,  please explain what could be the purpose to add those two layers:\n","metadata":{}},{"cell_type":"markdown","source":"\nDropout, deactivates some weights/connections for the trainings step that the network is more robust and don't learn into a local minima\n","metadata":{}},{"cell_type":"markdown","source":"\n\n\nLet's start coding the Network with the following functions:\n\n- Conv2D(filters/feature maps, kernel size, activation, padding, kernel_initializer)(input Data)\n- MaxPooling2D(pool_size = (x,x))(input Data)\n- Dropout(dropout rate)(input Data)\n- concatenate([input Data 1, Input Data 2], axis= x)\n- Conv2D()(UpSampling2D(size=(x,x))(Input Data)\n\nAs activation function we take 'relu' (Rectified Linear Unit).\nAs padding we chose 'same', thus despite convolution we get an output of the same size as the input.\nThe kernel initializer defines how we want to set the initial weights of this Keras layer. For example:\n- Zeros: initialize with a tensor of zeros\n- Ones: initialize with a tensor of ones\n- RandomNormal: initialize the tensor with a normal distribution\n- TruncatedNormal: initialize the tensor with a truncated normal distribution, where all values more than two standard deviations from its mean are neglected\n- he_normal: a zero mean Gaussian distribution with a standard deviation of \\sqrt{(2/n)}, where n is the number of inputs/connections of our weight tensor <br>\n\nWhich initialization do you think is beneficial?","metadata":{}},{"cell_type":"markdown","source":"\nhe_normal has shown good results, and bypasses dead neurons.","metadata":{}},{"cell_type":"markdown","source":"\n<br>\nWhen the architecture is configured correctly we have to define the inputs and outputs of our Neural Network with the method: <br> Model(inputs= ___ , outputs= __).\nThereafter we need to compile the whole architecture for training. Therefor we need to define the optimizer (with a proper learning rate) the loss function and we can evaluate define on which metrics we want to evaluate our model. \n\nList 3 loss functions and 3 optimizers and explain them:\n","metadata":{}},{"cell_type":"markdown","source":"\nLoss function:\n- mean squared error:\n- (squared) hinge:\n- crossentropy:\n- dice-loss\n- poisson:\n\nOptimizers:\n- Stochastic Gradient Descent:\n- RMSprop:\n- Adagrad:\n- Adam Optimizer:\n\n<br>\nChose one of these optimizers you thing fit best for your Unet and also a proper learning rate.\nThe return of our function should be the model.\n","metadata":{}},{"cell_type":"markdown","source":"## The Dice Coefficient Loss\n\nIn this section we will implement the Dice Coefficient loss, so we can use it with our own model.\nThe Dice Coefficient, also known as Sørensen–Dice coefficient compars the similarity of two quantities. When working with neural network these are, the prediction of our network and the ground truth. The dice coefficient can be interpreted as Intersection over union, so to speak points which appear in the predicition and ground truth, this makes it very attractive to use it on tasks with bounding boxes, image segmentation etc.\n<br>\nThe formular of the dice loss is given with:\n\\begin{align}\n    DSC = \\frac{2*|X \\cap Y|}{|X|+|Y|}\n\\end{align}\n\nBefor conding this formula you may think about:\n   - do I need the absolut values?\n   - how can i easily calculate the intersection, assuming that the ground truth is a one-hot vector:\n    \\begin{align}\n        y_{true} = \\begin{bmatrix}\n        0.0 &  0.0 &  0.0 &  1.0\n        \\end{bmatrix}\n    \\end{align}\n    \\begin{align}\n        y_{pred} = \\begin{bmatrix}\n        0.1 &  0.2 &  0.0 &  0.7\n        \\end{bmatrix}\n    \\end{align}\n<br>\n\nNow start with the coding, following these steps \n- First we want to flatten our input to the loss y_true and y_pred\n- Calculate the counter with its intersection\n- Calulate the denominator, (you may want to add a small epsilon to it)\n- Obtain the coefficient\n\nBecause we work here with the backend of keras, we need to do our operations with K.operation.\nK is the backend we imported above with \"from tensorflow.keras import backend as K\".\nHere a list of some operations is listed, if you need more, just google them:\n- sum: K.sum()\n- flatten: K.flatten()\n- mean: K.mean()\n- reshape: K.reshape()\n- dot: K.dot()\n- epsilon: K.epsilon()","metadata":{}},{"cell_type":"code","source":"def dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    coef = (2.0 * intersection + K.epsilon()) / (\n        K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon()\n    )\n    return coef","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Why it is advised to add an epsilon to your denominator?\n","metadata":{}},{"cell_type":"code","source":"def dice_coef_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##########################################################\n#           In this box some code must be added          #\n##########################################################\n\n\ndef unet(\n    pretrained_weights=None, input_size=(128, 128, 1)\n):  # the values given for the parameters are the default values\n    inputs = Input(input_size)\n    conv1 = Conv2D(\n        64, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(inputs)\n    conv1 = Conv2D(\n        64, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv2 = Conv2D(\n        128, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(pool1)\n    conv2 = Conv2D(\n        128, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = Conv2D(\n        256, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(pool2)\n\n    conv3 = Conv2D(\n        256, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv4 = Conv2D(\n        512, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(pool3)\n    conv4 = Conv2D(\n        512, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    conv5 = Conv2D(\n        1024, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(pool4)\n    conv5 = Conv2D(\n        1024, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(conv5)\n    drop5 = Dropout(0.5)(conv5)\n\n    up6 = Conv2D(\n        512, 2, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(UpSampling2D(size=(2, 2))(drop5))\n    merge6 = concatenate([drop4, up6], axis=3)\n    conv6 = Conv2D(\n        512, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(merge6)\n    conv6 = Conv2D(\n        512, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(conv6)\n\n    up7 = Conv2D(\n        256, 2, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(UpSampling2D(size=(2, 2))(conv6))\n    merge7 = concatenate([conv3, up7], axis=3)\n    conv7 = Conv2D(\n        256, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(merge7)\n    conv7 = Conv2D(\n        256, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(conv7)\n\n    up8 = Conv2D(\n        128, 2, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(UpSampling2D(size=(2, 2))(conv7))\n    merge8 = concatenate([conv2, up8], axis=3)\n    conv8 = Conv2D(\n        128, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(merge8)\n    conv8 = Conv2D(\n        128, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(conv8)\n\n    up9 = Conv2D(\n        64, 2, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(UpSampling2D(size=(2, 2))(conv8))\n    merge9 = concatenate([conv1, up9], axis=3)\n    conv9 = Conv2D(\n        64, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(merge9)\n    conv9 = Conv2D(\n        64, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(conv9)\n    conv9 = Conv2D(\n        2, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\"\n    )(conv9)\n\n    # In the last layer we chose the <sigmoid> as activation function because the output has to be between zero and one.\n    # We normalized out pixle values by deviding through 255.\n    # For our example we have only one class and thus only needing\n    # zero being white (our predicted object-segment) and 1 beeing black.\n    # but if we have more that one class the different colors will also lie inbetween 0 and 1 and every\n    # prediction is labeld with the appropriate number.\n    conv10 = Conv2D(1, 1, activation=\"sigmoid\")(conv9)\n\n    # With the following method <Model> we define out input and output of or Neural Network\n    # please complete:\n    model = Model(inputs=inputs, outputs=conv10)\n\n    # with Model.compile we configure and prepare our netwrok for training. Thus we define our optimizer for\n    # learning with it's learning rate, the loss function and the metrics.\n    model.compile(\n        optimizer=Adam(learning_rate=1e-4),\n        loss=\"binary_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n    # model.compile(optimizer=SGD(learning_rate=1e-4), loss=dice_coef_loss, metrics=['accuracy'])\n    # model.compile(optimizer=SGD(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n\n    # model.summary()\n\n    # this part we need for resume to a training or printing test, results for an already trained network, be careful\n    # that the input_size you defined for the model match with the one you defined for the model you take the pretrained\n    # weights from!\n    if pretrained_weights:\n        model.load_weights(pretrained_weights)\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparing the Data\n\nSo far we build the architecture of our neural net and the data, consists out of images and labels. To train the neural network in the next step the data has to be prepared.\n","metadata":{}},{"cell_type":"markdown","source":"#### Adjusting and Augmenting the Data\n\nBefore we feed the data to our neural network, it has to be adjusted properly. \nThe first step always helps is to normalize our input data. Important is to normalize both the labels and the images, so that the neural network does not learn something wrong, warped or even nothing at all.\nIn this tutorial we got the binary case and therefor the images and masks just need to be normalized.\nThis is done by dividing through 255. \nThe way we augment data in modern tensorflow is to use a Sequential model with augmentation layers, that return the input tensor in an augmented way. By using these layers we never need to convert our input tensors we created into a different format and can just later on run the input tensors and the labeled output tensors through the same model and get our augmented pair. \nIt is very important to use the same seed in both `` adjust_training_images `` and `` adjust_training_masks ``, because if we for example rotate an image, the corresponding laber has to be rotated in the same way.","metadata":{}},{"cell_type":"code","source":"##########################################################\n#           In this box some code must be added          #\n##########################################################\ndef adjust_training_images(\n    rotation_range: float,\n    width_shift_range: float,\n    height_shift_range: float,\n    zoom_range: float,\n    resize_height: int,\n    resize_width: int,\n    flip=\"horizontal\",\n    interpolation=\"nearest\",\n    seed=1,\n):\n\n    sequential_augmentation = Sequential(\n        [\n            tf.keras.layers.Rescaling(scale=1.0 / 255.0),\n            tf.keras.layers.RandomRotation(\n                factor=rotation_range, interpolation=interpolation, seed=seed\n            ),\n            tf.keras.layers.RandomZoom(\n                height_factor=zoom_range,\n                width_factor=zoom_range,\n                fill_mode=\"reflect\",\n                interpolation=interpolation,\n                seed=seed,\n            ),\n            tf.keras.layers.RandomFlip(mode=flip, seed=seed),\n            tf.keras.layers.RandomHeight(\n                factor=height_shift_range, interpolation=interpolation, seed=seed\n            ),\n            tf.keras.layers.RandomWidth(\n                factor=width_shift_range, interpolation=interpolation, seed=seed\n            ),\n            tf.keras.layers.Resizing(\n                resize_height,\n                resize_width,\n                interpolation=\"bilinear\",\n                crop_to_aspect_ratio=False,\n            ),\n        ]\n    )\n    return sequential_augmentation\n\n\ndef adjust_training_masks(\n    rotation_range: float,\n    width_shift_range: float,\n    height_shift_range: float,\n    zoom_range: float,\n    resize_height: int,\n    resize_width: int,\n    flip=\"horizontal\",\n    interpolation=\"nearest\",\n    seed=1,\n):\n\n    sequential_augmentation = Sequential(\n        [\n            tf.keras.layers.Rescaling(scale=1.0 / 255.0),\n            tf.keras.layers.RandomRotation(\n                factor=rotation_range, interpolation=interpolation, seed=seed\n            ),\n            tf.keras.layers.RandomZoom(\n                height_factor=zoom_range,\n                width_factor=zoom_range,\n                fill_mode=\"reflect\",\n                interpolation=interpolation,\n                seed=seed,\n            ),\n            tf.keras.layers.RandomFlip(mode=flip, seed=seed),\n            tf.keras.layers.RandomHeight(\n                factor=height_shift_range, interpolation=interpolation, seed=seed\n            ),\n            tf.keras.layers.RandomWidth(\n                factor=width_shift_range, interpolation=interpolation, seed=seed\n            ),\n            tf.keras.layers.Resizing(\n                resize_height,\n                resize_width,\n                interpolation=\"bilinear\",\n                crop_to_aspect_ratio=False,\n            ),\n        ]\n    )\n    return sequential_augmentation\n\n\nrescale_data = Sequential([tf.keras.layers.Rescaling(scale=1.0 / 255.0)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The training Generator\n\nThe next function will be the ``trainGenerator``.\nBecause deep learning task only get to learn something if they are trained with a lot of data, we have to provide it. But with the data we downloaded, there are only 30 images in the folder ``train/``. For a deep learning trask that is way to little. Therefor we will generating our own data with data augmentation. This function will use the data augmentation layers created above on the given images with layers like RandomRotation, RandomFlip and RandomZoom etc. Here the most important is that labels and images are transformed together with the same transformation functions. How good such an augmentation can replace more recordings strongly depends from the data on which we will learn. But for the given task the data augmentation works really good.\nThe function used here to generate the Data is the image_dataset_from_directory provided within Keras. The documentations for more information can be found here \nhttps://keras.io/preprocessing/image/.\n\nTo use the image augmentation function from above, we map the output onto the augmentation function. \n\nWhy is it do important that you will chose the same seed for the mask and the image itself and what does seed stand for?\n\n","metadata":{}},{"cell_type":"code","source":"def trainGenerator(\n    batch_size,\n    train_path,\n    image_folder,\n    mask_folder,\n    aug_dict,\n    image_color_mode=\"grayscale\",\n    mask_color_mode=\"grayscale\",\n    target_size=(128, 128),\n    seed=1,\n):\n    \"\"\"\n    can generate image and mask at the same time\n    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n    \"\"\"\n    # create two augmentation sequences with the same seed, to augment the images and the labels in the same way.\n    # This is crucial because otherwise the labels wouldn't fit the images anymore.\n    image_augmentation = adjust_training_images(**aug_dict)\n    mask_augmentation = adjust_training_masks(**aug_dict)\n\n    # Here we create a generator object and the map the augmentation and rescaling Sequences on the generators.\n    image_generator = (\n        image_dataset_from_directory(\n            train_path + \"/\" + image_folder,\n            labels=None,\n            label_mode=\"binary\",\n            color_mode=image_color_mode,\n            image_size=target_size,\n            batch_size=batch_size,\n            seed=seed,\n        )\n        .repeat()\n        .map(lambda x: image_augmentation(x))\n    )\n\n    mask_generator = (\n        image_dataset_from_directory(\n            train_path + \"/\" + mask_folder,\n            labels=None,\n            label_mode=\"binary\",\n            color_mode=mask_color_mode,\n            image_size=target_size,\n            batch_size=batch_size,\n            seed=seed,\n        )\n        .repeat()\n        .map(lambda x: mask_augmentation(x))\n    )\n\n    train_generator = zip(image_generator, mask_generator)\n    for (img, mask) in train_generator:\n        yield (img, mask)\n\n\n# to understand what yield is: https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The Testing Generator\n\nThe testGenerator function will generate your test data on the images provided within the test folder. \nIt will first read the images from the test-folder as gray images normalize them and resize them like defined in with the parameter target_size. Depending on the amount of object-classes you need the image vector will be extended. For our purpose again it is not necessary.  ","metadata":{}},{"cell_type":"code","source":"def testGenerator(test_path, image_color_mode=\"grayscale\", target_size=(128, 128)):\n\n    image_generator = image_dataset_from_directory(\n        test_path,\n        labels=None,\n        label_mode=\"binary\",\n        color_mode=image_color_mode,\n        image_size=target_size,\n        batch_size=1,\n        shuffle=False,\n    ).map(lambda x: (rescale_data(x)))\n\n    return image_generator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"#### The visualizer of the labels and predictions","metadata":{}},{"cell_type":"code","source":"def labelVisualize(num_class, img):\n    img = img[:, :, 0] if len(img.shape) == 3 else img\n    img_out = np.zeros(img.shape + (3,))\n    for i in range(num_class):\n        img_out[img == i, :] = color_dict[i]\n    return img_out / 255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Saving the results","metadata":{}},{"cell_type":"code","source":"def saveResult(save_path, npyfile, num_class=2):\n    for i, item in enumerate(npyfile):\n        img = np.array(item[:, :, 0] * 255, dtype=np.uint8)\n        io.imsave(os.path.join(save_path, \"%d_predict.png\" % i), img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run the training","metadata":{}},{"cell_type":"markdown","source":"Now all functions we need to run our Unet are defined and thus we can start the training.\n\nFirst we need to augmentate our dataset which contains up to now only 30 images.\nTherefor we can define some parameters within out data augmentation dictionary ``data_gen_args`` for this augmentation. To get some results and not just a gray image as prediciton, you should start with small parameters here.\n\nThen we will generate the data with the beforehand defined parameters and the images given in the training folder. \nThe first number hereby defines the batch_size, which you can change to a bigger number if your Computer fill in the requirements (GPU, Memory).\n\nNow it is time to built our UNet with just calling the function and than start the traing.\nYou can change the \"steps_per_epoch\" and \"epochs\". The more you chose the better the results will get but also the longer the training will take.\nThe Checkpoint-file will be saved within your Jupter Notebook folder.","metadata":{}},{"cell_type":"code","source":"data_gen_args = dict(\n    rotation_range=0.2,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    zoom_range=0.05,\n    resize_height=128,\n    resize_width=128,\n    flip=\"horizontal\",\n    interpolation=\"nearest\",\n)\n\nmyGene = trainGenerator(2, \"data/membrane/train\", \"image\", \"label\", data_gen_args)\n\nmodel = unet()\nmodel_checkpoint = ModelCheckpoint(\n    \"unet_membrane.hdf5\", monitor=\"loss\", verbose=1, save_best_only=True\n)\nmodel.fit(myGene, steps_per_epoch=30, epochs=1, callbacks=[model_checkpoint])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test your Model\n\nWhen the training is done you can generate some prediction on your test data and save them within the test-image folder.","metadata":{}},{"cell_type":"code","source":"testGene = testGenerator(\"data/membrane/test\", target_size=(128, 128))\nresults = model.predict(testGene, 30, verbose=1)\n\nif not os.path.exists(\"data/prediction\"):\n    os.mkdir(\"data/prediction\")\n\nsaveResult(\"data/prediction\", results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Show the results\n\nWe can now either look in the folder how well our network was trained or just show some images and their predicitons here:\n","metadata":{}},{"cell_type":"code","source":"test_path = \"data/membrane/test/\"\npredict_path = \"data/prediction/\"\n# number 0 to 29 your want to show\nimg_number = 1\ndisplay(Image(test_path + str(img_number) + \".png\"))\ndisplay(Image(predict_path + str(img_number) + \"_predict.png\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What's next\n\nSome ideas of what you can do next\n\n- Try on cars \n- try on dataset with multiple labels \n- try on zebrafish repository (https://osf.io/snb6p/)\n- adapt architecture (loss, learning-rate,...)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sources:<br>\n[Fig2](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)<br>\n[Fig3](https://arxiv.org/pdf/1505.04597.pdf) (from the paper) <br>","metadata":{}},{"cell_type":"markdown","source":"https://tuatini.me/practical-image-segmentation-with-unet/","metadata":{}}]}